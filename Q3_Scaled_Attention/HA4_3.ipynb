{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "4lni-h-cyNMt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "    - Q (np.array): Query matrix\n",
        "    - K (np.array): Key matrix\n",
        "    - V (np.array): Value matrix\n",
        "\n",
        "    Prints:\n",
        "    - Attention weights after softmax\n",
        "    - Final output after multiplying weights with V\n",
        "    \"\"\"\n",
        "    d_k = K.shape[1]\n",
        "\n",
        "    # Dot product of Q and transpose of K\n",
        "    scores = np.dot(Q, K.T)\n",
        "\n",
        "    # Scale scores by sqrt(d_k)\n",
        "    scaled_scores = scores / np.sqrt(d_k)\n",
        "\n",
        "    # Apply softmax to get attention weights\n",
        "    weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)\n",
        "\n",
        "    # Multiply weights by V to get output\n",
        "    output = np.dot(weights, V)\n",
        "\n",
        "    print(\"Attention Weights:\\n\", weights)\n",
        "    print(\"Output:\\n\", output)"
      ],
      "metadata": {
        "id": "WwJqV08wyNDV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Inputs\n",
        "Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])"
      ],
      "metadata": {
        "id": "TRze6vGvySH2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_dot_product_attention(Q, K, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7cmD46LyUVe",
        "outputId": "9f6fd738-4ac6-4f51-974b-344e5c0a65f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[0.73105858 0.26894142]\n",
            " [0.26894142 0.73105858]]\n",
            "Output:\n",
            " [[2.07576569 3.07576569 4.07576569 5.07576569]\n",
            " [3.92423431 4.92423431 5.92423431 6.92423431]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Why do we divide the attention score by √d in the scaled dot-product attention formula?\n",
        "Ans:\n",
        "\n",
        "\n",
        "*   We divide by √d (where d is the dimension of the key) to prevent the dot-product scores from becoming excessively large, which can push the softmax function into regions with tiny gradients, causing training instability. Scaling by √d keeps gradients stable, improving training performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "ynO381uZyenP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How does self-attention help the model understand relationships between words in a sentence?\n",
        "Ans:\n",
        "\n",
        "\n",
        "*   Self-attention allows each word in a sentence to weigh every other word, capturing dependencies and contextual relationships directly. This helps the model learn meaningful context, identifying the importance of words relative to one another, thereby improving understanding of the sentence's overall meaning.\n",
        "\n"
      ],
      "metadata": {
        "id": "p6yInE_KykdN"
      }
    }
  ]
}