# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12THk83flwbNmZvfKbzLetDy45U_J5nDK
"""

import numpy as np

def scaled_dot_product_attention(Q, K, V):
    """
    Compute scaled dot-product attention.

    Args:
    - Q (np.array): Query matrix
    - K (np.array): Key matrix
    - V (np.array): Value matrix

    Prints:
    - Attention weights after softmax
    - Final output after multiplying weights with V
    """
    d_k = K.shape[1]

    # Dot product of Q and transpose of K
    scores = np.dot(Q, K.T)

    # Scale scores by sqrt(d_k)
    scaled_scores = scores / np.sqrt(d_k)

    # Apply softmax to get attention weights
    weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)

    # Multiply weights by V to get output
    output = np.dot(weights, V)

    print("Attention Weights:\n", weights)
    print("Output:\n", output)

# Test Inputs
Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

scaled_dot_product_attention(Q, K, V)

"""#Why do we divide the attention score by √d in the scaled dot-product attention formula?
Ans:


*   We divide by √d (where d is the dimension of the key) to prevent the dot-product scores from becoming excessively large, which can push the softmax function into regions with tiny gradients, causing training instability. Scaling by √d keeps gradients stable, improving training performance.

#How does self-attention help the model understand relationships between words in a sentence?
Ans:


*   Self-attention allows each word in a sentence to weigh every other word, capturing dependencies and contextual relationships directly. This helps the model learn meaningful context, identifying the importance of words relative to one another, thereby improving understanding of the sentence's overall meaning.
"""