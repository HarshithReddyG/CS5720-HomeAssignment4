# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12THk83flwbNmZvfKbzLetDy45U_J5nDK
"""

!pip install transformers

from transformers import pipeline

# Load pre-trained sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis")

# Input sentence
sentence = "Despite the high price, the performance of the new MacBook is outstanding."

# Analyze sentiment
result = sentiment_pipeline(sentence)[0]

# Print results
print(f"Sentiment: {result['label']}")
print(f"Confidence Score: {result['score']:.4f}")

"""#What is the main architectural difference between BERT and GPT? Which uses an encoder and which uses a decoder?
Ans:
*   BERT uses a Transformer-based encoder architecture designed for understanding context from both directions (bi-directional).
*   GPT uses a Transformer-based decoder architecture, focusing on generating text sequentially from left to right (uni-directional).

#Explain why using pre-trained models (like BERT or GPT) is beneficial for NLP applications instead of training from scratch.
Ans:


*   Efficiency: Pre-trained models save significant computational resources and time since they have already learned general language patterns from large datasets.
*   Performance: They typically achieve better accuracy because they capture extensive linguistic knowledge, which helps them perform effectively even on relatively small domain-specific datasets.
"""