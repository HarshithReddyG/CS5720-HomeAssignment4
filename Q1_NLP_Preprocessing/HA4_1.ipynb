{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Install or download NLTK resources if needed"
      ],
      "metadata": {
        "id": "ZWv2Lbr7roVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smd3uMHhrtDK",
        "outputId": "d204e3e5-a453-4157-fe8a-e2b3521c048b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "O74eJz6Ir3-6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nlp_preprocessing_pipeline(sentence):\n",
        "    \"\"\"\n",
        "    Performs basic NLP preprocessing on the given sentence:\n",
        "    1. Tokenizes the sentence into individual words.\n",
        "    2. Removes common English stopwords.\n",
        "    3. Applies stemming to reduce each word to its root form.\n",
        "\n",
        "    Prints the following:\n",
        "    - A list of all tokens (original tokens)\n",
        "    - The list of tokens after stopwords are removed\n",
        "    - The final list after stemming\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens_no_stop = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens_no_stop]\n",
        "\n",
        "    # Print results\n",
        "    print(\"Original Tokens:\", tokens)\n",
        "    print(\"Tokens Without Stopwords:\", tokens_no_stop)\n",
        "    print(\"Stemmed Tokens:\", stemmed_tokens)"
      ],
      "metadata": {
        "id": "sl_bJCqPr88m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "nlp_preprocessing_pipeline(test_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_93P2_QysAPD",
        "outputId": "7f53d41f-5d39-4ca3-de7c-e11617cb27bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri', '.']\n",
            "Tokens Without Stopwords: ['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri', '.']\n",
            "Stemmed Tokens: ['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is the difference between stemming and lemmatization? Provide examples with the word “running.\n",
        "Ans:\n",
        "\n",
        "*   **Stemming**: uses simple, rule-based heuristics to chop off word endings (e.g., \"running\" → \"run\" or sometimes \"runn\"). It does not consider the context or part of speech, so it might produce non-dictionary forms.\n",
        "*   **Lemmatization**: is more sophisticated, using morphological analysis and vocabulary to convert a word to its base or dictionary form (lemma). For \"running,\" a lemmatizer (knowing it’s a verb) would typically return \"run.\"\n"
      ],
      "metadata": {
        "id": "8FCYApMZsVSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SQT6bFhztEwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Why might removing stop words be useful in some NLP tasks, and when might it actually be harmful?\n",
        "\n",
        "Ans:\n",
        "\n",
        "\n",
        "*   **Useful**: In tasks like text classification, topic modeling, or keyword extraction, removing common words (e.g., \"the\", \"in\", \"are\") helps reduce noise and focuses on more meaningful tokens.  \n",
        "*   **Harmful**: In sentiment analysis or other tasks where subtle linguistic cues matter, removing stop words can eliminate important context. For instance, negations like \"not\" drastically change meaning, so discarding them can lead to incorrect interpretations.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ay3ulUBatf3B"
      }
    }
  ]
}